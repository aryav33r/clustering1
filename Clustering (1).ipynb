{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical**"
      ],
      "metadata": {
        "id": "VJjeb9KVhF6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. What is unsupervised learning in the context of machine learning?\n",
        "Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabeled data without any predefined output labels or human supervision. Unlike supervised learning, there are no correct answers or target variables provided to the model. The system tries to learn the underlying structure, patterns, or relationships in the data on its own. Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of variables), and anomaly detection (identifying unusual data points). Unsupervised learning is particularly useful when you don't know what you're looking for in the data or when labeling data would be too expensive or time-consuming.\n",
        "\n",
        "## 2. How does K-Means clustering algorithm work?\n",
        "K-Means is an iterative clustering algorithm that partitions data into K distinct, non-overlapping clusters. Here's how it works in detail:\n",
        "\n",
        "1. **Initialization**: Randomly select K data points as initial cluster centroids (or use a smarter initialization method like K-Means++).\n",
        "\n",
        "2. **Assignment Step**: For each data point, calculate the distance (usually Euclidean) to all centroids and assign the point to the nearest centroid's cluster.\n",
        "\n",
        "3. **Update Step**: Recalculate the centroids by taking the mean of all data points assigned to each cluster.\n",
        "\n",
        "4. **Iteration**: Repeat the assignment and update steps until either:\n",
        "   - The centroids no longer change significantly (convergence)\n",
        "   - A maximum number of iterations is reached\n",
        "   - The within-cluster sum of squares (inertia) stops decreasing significantly\n",
        "\n",
        "The algorithm aims to minimize the within-cluster sum of squares (inertia), which measures how tightly grouped the points in each cluster are.\n",
        "\n",
        "## 3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "A dendrogram is a tree-like diagram that records the sequences of merges or splits in hierarchical clustering. It provides a visual representation of the hierarchical relationships between data points and clusters. Key features:\n",
        "\n",
        "- **Leaves**: The bottom of the dendrogram represents individual data points.\n",
        "- **Nodes**: Points where clusters merge, showing which clusters or points are joined.\n",
        "- **Height/Y-axis**: Represents the distance or dissimilarity between merging clusters. The higher the merge point, the more dissimilar the clusters were when they merged.\n",
        "- **Cutting the dendrogram**: Drawing a horizontal line at a certain height determines the number of clusters - the line will intersect the dendrogram at K points, giving K clusters.\n",
        "\n",
        "Dendrograms are useful for understanding the natural hierarchy in data and determining an appropriate number of clusters by examining where large \"jumps\" in merge distances occur.\n",
        "\n",
        "## 4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "The main differences are:\n",
        "\n",
        "1. **Approach**:\n",
        "   - K-Means is a partitional clustering algorithm that divides data into K distinct clusters all at once.\n",
        "   - Hierarchical clustering builds a hierarchy of clusters either through:\n",
        "     * Agglomerative (bottom-up) approach: Starts with each point as its own cluster and merges them\n",
        "     * Divisive (top-down) approach: Starts with all points in one cluster and splits them\n",
        "\n",
        "2. **Number of clusters**:\n",
        "   - K-Means requires specifying K (number of clusters) beforehand.\n",
        "   - Hierarchical clustering doesn't require pre-specifying K (you can decide later by cutting the dendrogram).\n",
        "\n",
        "3. **Result structure**:\n",
        "   - K-Means gives a flat set of clusters.\n",
        "   - Hierarchical clustering gives a nested tree of clusters (dendrogram).\n",
        "\n",
        "4. **Flexibility**:\n",
        "   - K-Means creates spherical clusters of roughly equal size.\n",
        "   - Hierarchical clustering can reveal more complex cluster structures.\n",
        "\n",
        "5. **Performance**:\n",
        "   - K-Means is generally faster (O(n)) for large datasets.\n",
        "   - Hierarchical clustering is more computationally expensive (O(n² or n³)).\n",
        "\n",
        "## 5. What are the advantages of DBSCAN over K-Means?\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has several advantages over K-Means:\n",
        "\n",
        "1. **No need to specify number of clusters**: DBSCAN automatically determines the number of clusters based on data density.\n",
        "\n",
        "2. **Handles arbitrary cluster shapes**: Unlike K-Means which finds spherical clusters, DBSCAN can find clusters of any shape.\n",
        "\n",
        "3. **Robust to outliers**: DBSCAN explicitly models noise points, while K-Means forces all points into clusters.\n",
        "\n",
        "4. **Handles varying densities**: With appropriate parameter settings, DBSCAN can find clusters of different densities.\n",
        "\n",
        "5. **Works well with spatial data**: DBSCAN is particularly effective for geospatial data or data where density matters.\n",
        "\n",
        "6. **Deterministic results**: For core points, DBSCAN's results are deterministic (unlike K-Means which depends on random initialization).\n",
        "\n",
        "7. **No assumption of cluster size**: K-Means tends to create clusters of similar size, while DBSCAN doesn't have this bias.\n",
        "\n",
        "However, DBSCAN requires careful tuning of its parameters (eps and min_samples) and struggles with high-dimensional data or data with significantly varying densities.\n",
        "\n",
        "## 6. When would you use Silhouette Score in clustering?\n",
        "The Silhouette Score is used in these scenarios:\n",
        "\n",
        "1. **Evaluating clustering quality**: When you want to assess how well-separated the clusters are and how appropriately each point has been assigned to its cluster.\n",
        "\n",
        "2. **Determining optimal number of clusters**: When using methods like K-Means where you need to choose K, the Silhouette Score can help identify the K that produces the most distinct clusters.\n",
        "\n",
        "3. **Comparing different clustering algorithms**: To objectively compare the results of different clustering approaches on the same data.\n",
        "\n",
        "4. **Validating cluster assignments**: When you need to check if points are consistently closer to their own cluster than to other clusters.\n",
        "\n",
        "The score ranges from -1 to 1, where:\n",
        "- 1 indicates perfect clustering (points are very close to their cluster and far from others)\n",
        "- 0 indicates overlapping clusters\n",
        "- -1 indicates incorrect clustering\n",
        "\n",
        "It's particularly useful when ground truth labels aren't available (which is typical in unsupervised learning).\n",
        "\n",
        "## 7. What are the limitations of Hierarchical Clustering?\n",
        "Hierarchical clustering has several limitations:\n",
        "\n",
        "1. **Computational complexity**:\n",
        "   - Agglomerative: O(n³) time and O(n²) memory for standard implementations\n",
        "   - Divisive: Even more computationally expensive\n",
        "   - Becomes impractical for large datasets (>10,000 points)\n",
        "\n",
        "2. **Sensitivity to noise and outliers**: A few outliers can significantly affect the merging process and resulting hierarchy.\n",
        "\n",
        "3. **Difficulty with updates**: Adding new data points requires recomputing the entire hierarchy.\n",
        "\n",
        "4. **Irreversible decisions**: Once clusters are merged or split, the decision cannot be undone in subsequent steps.\n",
        "\n",
        "5. **Difficulty choosing the \"right\" clusters**: Determining where to cut the dendrogram can be subjective.\n",
        "\n",
        "6. **Memory intensive**: Needs to store the full distance/similarity matrix for agglomerative clustering.\n",
        "\n",
        "7. **Not optimized for globular clusters**: Like K-Means, it often performs poorly with non-convex cluster shapes.\n",
        "\n",
        "8. **Linkage criteria impact**: Different linkage methods (single, complete, average, Ward's) can produce very different results.\n",
        "\n",
        "9. **Difficulty scaling**: Doesn't work well with very large datasets due to memory and computational constraints.\n",
        "\n",
        "## 8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "Feature scaling is crucial for K-Means and many other clustering algorithms because:\n",
        "\n",
        "1. **Distance-based sensitivity**: K-Means uses distance metrics (typically Euclidean) to assign points to clusters. Features with larger scales will dominate the distance calculation, making the algorithm effectively ignore features with smaller scales.\n",
        "\n",
        "2. **Centroid calculation**: Since centroids are means of feature values, unscaled features with larger ranges will have a greater influence on centroid positions.\n",
        "\n",
        "3. **Convergence issues**: Without scaling, the algorithm might take longer to converge or get stuck in poor local optima.\n",
        "\n",
        "4. **Comparable features**: Scaling ensures all features contribute equally to the similarity measure, which is especially important when features represent different units or measurement scales.\n",
        "\n",
        "Common scaling methods include:\n",
        "- Standardization (subtract mean, divide by std dev) - good when data is normally distributed\n",
        "- Min-Max scaling (scale to [0,1] range) - good for bounded data\n",
        "- Robust scaling (uses median and IQR) - good for data with outliers\n",
        "\n",
        "Without scaling, clustering results can be misleading and dominated by the highest-variance features regardless of their actual importance.\n",
        "\n",
        "## 9. How does DBSCAN identify noise points?\n",
        "DBSCAN identifies noise points (also called outliers) through the following process:\n",
        "\n",
        "1. **Core points**: A point is a core point if at least min_samples points (including itself) are within its ε (eps) neighborhood.\n",
        "\n",
        "2. **Border points**: A point that is not a core point but is within the ε neighborhood of a core point.\n",
        "\n",
        "3. **Noise points**: Any point that is neither a core point nor a border point is considered noise.\n",
        "\n",
        "In other words:\n",
        "- Noise points have fewer than min_samples points in their ε neighborhood.\n",
        "- None of these points are within ε distance of any core point.\n",
        "- They don't belong to any cluster's dense region.\n",
        "\n",
        "These points are labeled as -1 in DBSCAN's output. The identification of noise is automatic and doesn't require a separate outlier detection step, which is one of DBSCAN's advantages over methods like K-Means that force all points into clusters.\n",
        "\n",
        "## 10. Define inertia in the context of K-Means.\n",
        "Inertia, in K-Means clustering, is the sum of squared distances of samples to their closest cluster center. It's also called within-cluster sum of squares (WCSS). Mathematically, it's defined as:\n",
        "\n",
        "Inertia = Σ (for all points i) distance(x_i, centroid of cluster containing x_i)²\n",
        "\n",
        "Where:\n",
        "- x_i is a data point\n",
        "- The distance is typically Euclidean distance\n",
        "- The centroid is the mean of all points in the cluster\n",
        "\n",
        "Key properties:\n",
        "1. **Optimization target**: K-Means tries to minimize inertia during its iterative process.\n",
        "2. **Cluster compactness**: Lower inertia means tighter, more compact clusters.\n",
        "3. **Limitation**: Inertia assumes clusters are convex and isotropic, and isn't reliable for comparing across different numbers of clusters or datasets.\n",
        "4. **Elbow method**: Used to determine optimal K by looking for the \"elbow\" where inertia stops decreasing significantly.\n",
        "\n",
        "Inertia tends to decrease as K increases (with K=n giving zero inertia but meaningless clusters), so it can't be used alone to choose K.\n",
        "\n",
        "## 11. What is the elbow method in K-Means clustering?\n",
        "The elbow method is a heuristic used to determine the optimal number of clusters (K) in K-Means clustering. Here's how it works:\n",
        "\n",
        "1. **Process**:\n",
        "   - Run K-Means for a range of K values (typically from 1 to some reasonable maximum)\n",
        "   - For each K, calculate the inertia (within-cluster sum of squares)\n",
        "   - Plot inertia against K\n",
        "\n",
        "2. **Identifying the elbow**:\n",
        "   - As K increases, inertia decreases (more clusters can fit the data better)\n",
        "   - Look for the point where the rate of decrease sharply changes (the \"elbow\")\n",
        "   - This point suggests diminishing returns from increasing K further\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - Before the elbow: Adding clusters significantly improves fit\n",
        "   - After the elbow: Additional clusters provide marginal improvement\n",
        "   - The elbow K is often chosen as optimal\n",
        "\n",
        "4. **Limitations**:\n",
        "   - Sometimes no clear elbow exists\n",
        "   - Subjective to identify exactly where the elbow is\n",
        "   - Doesn't work well when data has overlapping clusters\n",
        "   - Inertia always decreases with K, making very large K seem better\n",
        "\n",
        "Often used in conjunction with other metrics like silhouette score for more robust K selection.\n",
        "\n",
        "## 12. Describe the concept of \"density\" in DBSCAN.\n",
        "In DBSCAN, \"density\" refers to the concentration of data points in a particular region of the feature space. The algorithm's core idea is that clusters are dense regions separated by less dense regions. Key aspects:\n",
        "\n",
        "1. **Density parameters**:\n",
        "   - ε (eps): Radius of the neighborhood around a point\n",
        "   - min_samples: Minimum number of points required to form a dense region\n",
        "\n",
        "2. **Density definitions**:\n",
        "   - A point is in a dense region if at least min_samples points are within ε distance\n",
        "   - A cluster is a maximal set of density-connected points\n",
        "   - Density-reachable: A point q is density-reachable from p if there's a chain of points where each is within ε of the next and all have min_samples neighbors\n",
        "\n",
        "3. **Density-based clustering**:\n",
        "   - Discovers clusters of arbitrary shape by connecting dense regions\n",
        "   - Can separate clusters by sparse regions\n",
        "   - Identifies outliers as points in low-density regions\n",
        "\n",
        "4. **Advantages of density-based approach**:\n",
        "   - Doesn't assume spherical clusters like K-Means\n",
        "   - Can find clusters of varying shapes and sizes\n",
        "   - Naturally handles noise/outliers in sparse regions\n",
        "\n",
        "The density concept makes DBSCAN particularly effective for spatial data and datasets with irregular cluster shapes.\n",
        "\n",
        "## 13. Can hierarchical clustering be used on categorical data?\n",
        "Yes, hierarchical clustering can be used with categorical data, but with some important considerations:\n",
        "\n",
        "1. **Distance metrics**: Need to use appropriate dissimilarity measures for categorical data:\n",
        "   - Hamming distance: Fraction of features that differ\n",
        "   - Jaccard distance: For sets of categories\n",
        "   - Simple matching coefficient: For binary categorical data\n",
        "   - Gower's distance: Can handle mixed data types\n",
        "\n",
        "2. **Linkage methods**: Some linkage methods work better than others:\n",
        "   - Single linkage can lead to chaining\n",
        "   - Complete or average linkage are often better choices\n",
        "\n",
        "3. **Preprocessing**:\n",
        "   - May need to one-hot encode or use other encoding schemes\n",
        "   - Some implementations require numeric input\n",
        "\n",
        "4. **Limitations**:\n",
        "   - Results can be sensitive to the distance metric chosen\n",
        "   - Some information may be lost in encoding categorical variables\n",
        "   - Interpretation can be more challenging than with numerical data\n",
        "\n",
        "5. **Alternatives**:\n",
        "   - For purely categorical data, specific algorithms like ROCK or COOLCAT may be more appropriate\n",
        "   - For mixed data, consider Gower's distance with hierarchical clustering\n",
        "\n",
        "The key is choosing an appropriate dissimilarity measure that properly captures the relationships between categorical values.\n",
        "\n",
        "## 14. What does a negative Silhouette Score indicate?\n",
        "A negative Silhouette Score indicates that:\n",
        "\n",
        "1. **Poor clustering**: On average, data points are closer to points in other clusters than to points in their own cluster.\n",
        "\n",
        "2. **Possible interpretations**:\n",
        "   - The number of clusters may be incorrect (too many or too few)\n",
        "   - The clustering algorithm may be inappropriate for the data structure\n",
        "   - The data may not have meaningful cluster structure\n",
        "   - The distance metric may not be suitable for the data\n",
        "\n",
        "3. **Specific meaning**:\n",
        "   - The average distance between a point and points in other clusters (a) is less than the average distance to points in its own cluster (b)\n",
        "   - This suggests points are assigned to the \"wrong\" clusters\n",
        "\n",
        "4. **Actions to consider**:\n",
        "   - Try a different number of clusters\n",
        "   - Consider a different clustering algorithm\n",
        "   - Re-examine feature scaling or preprocessing\n",
        "   - Check if the data actually contains meaningful clusters\n",
        "   - Try a different distance metric if appropriate\n",
        "\n",
        "While positive scores indicate good clustering (higher is better), negative scores suggest the current clustering configuration is worse than random assignment.\n",
        "\n",
        "## 15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "Linkage criteria determine how the distance between two clusters is computed during hierarchical clustering. Different criteria lead to different cluster structures. Common linkage methods:\n",
        "\n",
        "1. **Single linkage** (nearest neighbor):\n",
        "   - Distance between clusters = minimum distance between any two points in different clusters\n",
        "   - Tends to produce \"chaining\" - long, elongated clusters\n",
        "   - Sensitive to noise and outliers\n",
        "\n",
        "2. **Complete linkage** (farthest neighbor):\n",
        "   - Distance = maximum distance between any two points in different clusters\n",
        "   - Tends to produce compact, spherical clusters of similar size\n",
        "   - Less sensitive to noise but can break large clusters\n",
        "\n",
        "3. **Average linkage**:\n",
        "   - Distance = average distance between all pairs of points in different clusters\n",
        "   - Compromise between single and complete linkage\n",
        "   - Less sensitive to outliers than single linkage\n",
        "\n",
        "4. **Ward's method** (minimum variance):\n",
        "   - Minimizes the total within-cluster variance\n",
        "   - Merges clusters that lead to smallest increase in total variance\n",
        "   - Tends to produce clusters of similar size\n",
        "   - Works well with Euclidean distance\n",
        "\n",
        "5. **Centroid linkage**:\n",
        "   - Distance = distance between cluster centroids\n",
        "   - Can lead to inversion in dendrograms (where later merges occur at lower distances than earlier ones)\n",
        "\n",
        "The choice of linkage affects:\n",
        "- The shape and size of resulting clusters\n",
        "- The interpretation of the dendrogram\n",
        "- The algorithm's sensitivity to noise\n",
        "- The computational complexity\n",
        "\n",
        "## 16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
        "K-Means tends to perform poorly with varying cluster sizes or densities because:\n",
        "\n",
        "1. **Equal-size assumption**: K-Means implicitly assumes clusters are roughly equally sized, as it assigns points to the nearest centroid without considering cluster density or size.\n",
        "\n",
        "2. **Spherical cluster bias**: The algorithm works best when clusters are spherical and equally dense, as it uses Euclidean distance which spreads equally in all directions.\n",
        "\n",
        "3. **Centroid influence**: In clusters with different densities:\n",
        "   - Dense clusters may be split to assign points to sparse clusters\n",
        "   - Sparse clusters may be merged into nearby dense clusters\n",
        "\n",
        "4. **Distance metric limitation**: Euclidean distance favors equal-sized clusters since points at the same distance belong to the nearest centroid regardless of local density.\n",
        "\n",
        "5. **Initialization sensitivity**: With uneven cluster sizes, random initialization is more likely to place initial centroids in large clusters, ignoring smaller ones.\n",
        "\n",
        "6. **Variance sensitivity**: K-Means minimizes within-cluster variance, which can lead to:\n",
        "   - Splitting large, loose clusters\n",
        "   - Merging small, tight clusters\n",
        "\n",
        "7. **Example failure cases**:\n",
        "   - One large, diffuse cluster and one small, dense cluster\n",
        "   - Clusters with very different numbers of points\n",
        "   - Adjacent clusters with different densities\n",
        "\n",
        "Alternatives like DBSCAN or Gaussian Mixture Models often perform better on such data.\n",
        "\n",
        "## 17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "DBSCAN has two core parameters:\n",
        "\n",
        "1. **eps (ε)**:\n",
        "   - The maximum distance between two points for one to be considered in the neighborhood of the other\n",
        "   - Influences:\n",
        "     * Cluster size: Larger ε leads to larger clusters (more points are reachable)\n",
        "     * Number of clusters: Larger ε may merge separate clusters\n",
        "     * Noise points: Larger ε may convert noise to border points\n",
        "   - Too small: Many small clusters and noise\n",
        "   - Too large: Few large clusters, may merge distinct groups\n",
        "\n",
        "2. **min_samples**:\n",
        "   - Minimum number of points required to form a dense region (core point)\n",
        "   - Influences:\n",
        "     * Cluster formation: Higher values require more points to start a cluster\n",
        "     * Noise sensitivity: Higher values make algorithm more robust to noise\n",
        "     * Cluster granularity: Higher values find only very dense regions as clusters\n",
        "   - Too small: Many small clusters including around noise\n",
        "   - Too large: May miss legitimate smaller clusters\n",
        "\n",
        "Choosing parameters:\n",
        "- For ε: Look at k-distance plot (distance to k=min_samples-th neighbor)\n",
        "- For min_samples: Depends on data size and expected cluster density (often start with 2*dimensions)\n",
        "- Domain knowledge about expected cluster density helps\n",
        "\n",
        "Additional considerations:\n",
        "- Distance metric choice affects ε interpretation\n",
        "- Data scaling is crucial as ε is sensitive to feature scales\n",
        "- Higher dimensions require larger min_samples due to curse of dimensionality\n",
        "\n",
        "## 18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "K-Means++ improves standard K-Means initialization by providing a smarter method for selecting initial centroids, addressing these issues with random initialization:\n",
        "\n",
        "1. **Standard K-Means problems**:\n",
        "   - Random initialization can lead to poor clusterings\n",
        "   - Centroids might start too close together\n",
        "   - Some clusters might get no initial centroids\n",
        "   - Requires more runs to get good results\n",
        "\n",
        "2. **K-Means++ algorithm**:\n",
        "   a. Choose first centroid uniformly at random from data points\n",
        "   b. For each subsequent centroid:\n",
        "      i. Compute D(x), the distance from each point to nearest existing centroid\n",
        "      ii. Choose new centroid with probability proportional to D(x)²\n",
        "      iii. Repeat until K centroids are chosen\n",
        "   c. Proceed with standard K-Means\n",
        "\n",
        "3. **Advantages**:\n",
        "   - Initial centroids are spread out, covering the dataset better\n",
        "   - Higher probability of picking centroids in different clusters\n",
        "   - Leads to more consistent and better final clusterings\n",
        "   - Often converges faster with fewer iterations\n",
        "   - Reduces need for multiple random initializations\n",
        "\n",
        "4. **Theoretical guarantees**:\n",
        "   - Expected approximation ratio of O(log k) to optimal clustering\n",
        "   - In practice, often finds better solutions than random initialization\n",
        "\n",
        "5. **Practical impact**:\n",
        "   - Typically requires fewer iterations to converge\n",
        "   - Produces more stable results across different runs\n",
        "   - Especially helpful when K is large compared to number of points\n",
        "\n",
        "## 19. What is agglomerative clustering?\n",
        "Agglomerative clustering is a bottom-up hierarchical clustering approach where:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Start with each data point as its own cluster (n clusters)\n",
        "\n",
        "2. **Iterative process**:\n",
        "   a. Compute pairwise distances between all clusters\n",
        "   b. Merge the two closest clusters based on linkage criterion\n",
        "   c. Update distance matrix to reflect new cluster\n",
        "   d. Repeat until all points are in one cluster\n",
        "\n",
        "3. **Key components**:\n",
        "   - **Distance metric**: How to measure distance between points (Euclidean, Manhattan, etc.)\n",
        "   - **Linkage criterion**: How to define distance between clusters (single, complete, average, Ward's)\n",
        "\n",
        "4. **Output**:\n",
        "   - Produces a dendrogram showing the complete merging history\n",
        "   - Can be cut at any level to obtain a specific number of clusters\n",
        "\n",
        "5. **Characteristics**:\n",
        "   - Deterministic (unlike K-Means with random initialization)\n",
        "   - Reveals hierarchical structure in data\n",
        "   - Doesn't require pre-specifying number of clusters\n",
        "   - More interpretable than flat clustering methods\n",
        "\n",
        "6. **Variants**:\n",
        "   - **Single linkage**: Minimum distance between clusters (can lead to chaining)\n",
        "   - **Complete linkage**: Maximum distance (creates compact clusters)\n",
        "   - **Average linkage**: Average distance between points\n",
        "   - **Ward's method**: Minimizes variance when merging\n",
        "\n",
        "7. **Applications**:\n",
        "   - When hierarchical relationships are important\n",
        "   - For small to medium datasets\n",
        "   - When you want to explore data at multiple granularities\n",
        "\n",
        "## 20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "The Silhouette Score is often more informative than inertia because:\n",
        "\n",
        "1. **Relative vs absolute measure**:\n",
        "   - Inertia is an absolute measure of cluster compactness\n",
        "   - Silhouette Score measures both cluster cohesion (how close points are to their cluster) and separation (how far points are from other clusters)\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Silhouette Score is normalized between -1 and 1, making it comparable across different datasets and scales\n",
        "   - Inertia values are dataset-specific and not directly comparable\n",
        "\n",
        "3. **Cluster separation**:\n",
        "   - Inertia only measures within-cluster distances\n",
        "   - Silhouette considers both within-cluster and between-cluster distances\n",
        "\n",
        "4. **Number of clusters**:\n",
        "   - Inertia always decreases as K increases, making it hard to choose K\n",
        "   - Silhouette Score can indicate when adding clusters doesn't improve separation\n",
        "\n",
        "5. **Interpretability**:\n",
        "   - Silhouette values:\n",
        "     * Near 1: Points are well-clustered\n",
        "     * Near 0: Points are on/very near cluster boundaries\n",
        "     * Negative: Points may be in wrong cluster\n",
        "   - Inertia has no such direct interpretation\n",
        "\n",
        "6. **Shape awareness**:\n",
        "   - Silhouette can work better with non-spherical clusters\n",
        "   - Inertia assumes spherical clusters due to centroid-based measurement\n",
        "\n",
        "7. **Practical advantages**:\n",
        "   - Helps identify poorly clustered points\n",
        "   - Can compare different clustering algorithms\n",
        "   - Works better for choosing K in many cases\n",
        "\n",
        "However, Silhouette Score is more computationally expensive to calculate (O(n²)) and may not work well with very large datasets."
      ],
      "metadata": {
        "id": "A4SUwXzjhVfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **practical**"
      ],
      "metadata": {
        "id": "e9ITqnqzhgaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
        "scatter plot\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X')\n",
        "plt.title(\"K-Means Clustering with 4 Centers\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kZP0Zz0b-rRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10\n",
        "predicted labels ?\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "y_pred = agg.fit_predict(X)\n",
        "\n",
        "print(\"First 10 predicted labels:\", y_pred[:10])"
      ],
      "metadata": {
        "id": "O1Foh_3d-w5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"DBSCAN on Moons Dataset (Outliers shown in purple)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h45aSjqEtrWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each\n",
        "cluster\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "y_pred = kmeans.predict(X_scaled)\n",
        "\n",
        "from collections import Counter\n",
        "print(\"Cluster sizes:\", Counter(y_pred))"
      ],
      "metadata": {
        "id": "vglSAuwwucom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"DBSCAN on Circles Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kwweb4iouoSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster\n",
        "centroids\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)"
      ],
      "metadata": {
        "id": "c2Gjuw4xuzfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27.  Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with\n",
        "DBSCAN\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"DBSCAN on Blobs with Varying STD\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Fa0D8eau95D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='tab10')\n",
        "plt.title(\"K-Means Clusters on Digits PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OIVkfWfhvK6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29.  Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "scores = []\n",
        "k_values = range(2, 6)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    y_pred = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, y_pred)\n",
        "    scores.append(score)\n",
        "\n",
        "plt.bar(k_values, scores)\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Scores for Different k Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iimv41jRvWPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 .  Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "Z = linkage(X, method='average')\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.title(\"Dendrogram with Average Linkage (Iris Dataset)\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VsLjRfGxvjDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with\n",
        "decision boundaries\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=2.0, random_state=42)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "# Create mesh grid\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"K-Means with Decision Boundaries\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6qGEfntpvvsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=5, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='tab10')\n",
        "plt.title(\"DBSCAN Clustering after t-SNE\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xxq6uybOv7HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33.  Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot\n",
        "the result\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "y_pred = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"Agglomerative Clustering with Complete Linkage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZI-dhYCvwHzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34.  Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a\n",
        "line plot\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "inertias = []\n",
        "k_values = range(2, 7)\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(k_values, inertias, marker='o')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v63iP4bPwQI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35.  Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with\n",
        "single linkage .\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "y_pred = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering with Single Linkage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pT6KeHM3wrq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36.  Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding\n",
        "noise)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
        "print(f\"Number of clusters (excluding noise): {n_clusters}\")\n"
      ],
      "metadata": {
        "id": "XShOQ9Mhw172"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37.  Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the\n",
        "data points\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.5)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "            s=200, c='red', marker='X', label='Cluster Centers')\n",
        "plt.title('K-Means Clustering with Cluster Centers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fg002z8ExFkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "y_pred = dbscan.fit_predict(X)\n",
        "\n",
        "n_noise = list(y_pred).count(-1)\n",
        "print(f\"Number of noise samples: {n_noise}\")"
      ],
      "metadata": {
        "id": "N_ndHHdxxNgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the\n",
        "clustering result\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title('K-Means on Non-linearly Separable Data (Moons)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0JrfCWYoxbNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D\n",
        "scatter plot.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X_pca)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y_pred, cmap='tab10', s=20)\n",
        "ax.set_title('K-Means Clustering on Digits (3D PCA)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u89fcW30xkH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the\n",
        "clustering\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, y_pred)\n",
        "print(f\"Silhouette Score: {score:.3f}\")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(f\"K-Means Clustering (Silhouette Score: {score:.3f})\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_o2EwKUQxuUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering.\n",
        "Visualize in 2D\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "y_pred = agg.fit_predict(X_pca)\n",
        "\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"Agglomerative Clustering on PCA-reduced Breast Cancer Data\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQFCYx4Oyo82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN\n",
        "side-by-side\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "# DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis')\n",
        "plt.title(\"KMeans Clustering\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='viridis')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QQEcp8Byw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "silhouette_vals = silhouette_samples(X, y_pred)\n",
        "y_lower = 10\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(3):\n",
        "    cluster_silhouette_vals = silhouette_vals[y_pred == i]\n",
        "    cluster_silhouette_vals.sort()\n",
        "    y_upper = y_lower + cluster_silhouette_vals.shape[0]\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                     0, cluster_silhouette_vals,\n",
        "                     alpha=0.7)\n",
        "    plt.text(-0.05, y_lower + 0.5 * cluster_silhouette_vals.shape[0], str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Silhouette Plot for Iris Dataset\")\n",
        "plt.xlabel(\"Silhouette Coefficient Values\")\n",
        "plt.ylabel(\"Cluster Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HBk2INoAy7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage.\n",
        "Visualize clusters\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "y_pred = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n",
        "plt.title(\"Agglomerative Clustering with Average Linkage\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIhBeA1HzKV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4\n",
        "features)\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "# Create DataFrame with first 4 features and cluster assignments\n",
        "df = pd.DataFrame(X[:, :4], columns=wine.feature_names[:4])\n",
        "df['Cluster'] = y_pred\n",
        "\n",
        "sns.pairplot(df, hue='Cluster', palette='viridis')\n",
        "plt.suptitle(\"Pairplot of Wine Data with KMeans Clusters\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZqVhFB-qzSqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the\n",
        "count\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
        "dbscan = DBSCAN(eps=0.8, min_samples=10)\n",
        "y_pred = dbscan.fit_predict(X)\n",
        "\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    if label == -1:\n",
        "        print(f\"Noise points: {count}\")\n",
        "    else:\n",
        "        print(f\"Cluster {label} size: {count}\")"
      ],
      "metadata": {
        "id": "TiaV0H3KzahP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 48.  Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the\n",
        "clusters\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "y_pred = agg.fit_predict(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='tab10', s=10)\n",
        "plt.title(\"Agglomerative Clustering on t-SNE reduced Digits Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P6VwQ13RziIz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}